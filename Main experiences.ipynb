{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8LLkD_298Jk"
      },
      "source": [
        "This notebook can only be run after running the \"Models Preparation\" notebook, which pre-computes the models.\r\n",
        "\r\n",
        "After loading the models, preparing the data and pre-computing what is needed, this notebook will compute the different experiences of the article. It is possible to choose the BATS categories for computing the error terms norms as well as the paraphrase error rankings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBSJ-c8aghJf"
      },
      "source": [
        "import gensim\n",
        "from gensim import utils, matutils\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "import math\n",
        "import time\n",
        "import logging\n",
        "from itertools import chain\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "import scipy\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
        "\n",
        "from scipy.stats import gaussian_kde\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaP5JkGcghJp"
      },
      "source": [
        "# Loading models and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Aw4JMcdghJq"
      },
      "source": [
        "def save_obj(obj, name ):\n",
        "    with open( name + '.pkl', 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_obj(name ):\n",
        "    with open(name + '.pkl', 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "text8 = api.load(\"text8\")\n",
        "    \n",
        "# Make sure to have run the Models preparation.ipynb file before to prepare the different models and count matrices.\n",
        "\n",
        "count_dict_ij = pickle.load( open( \"./Models/count_dict_ij.pkl\", \"rb\" ) )\n",
        "count_matrix_tri = scipy.sparse.load_npz('./Models/count_matrix_tri.npz')\n",
        "pmi_matrix = scipy.sparse.load_npz('./Models/pmi_matrix_nominctxt.npz')\n",
        "para_matrix = scipy.sparse.load_npz('./Models/paraphrase_matrix.npz')\n",
        "model = Word2Vec.load(\"./Models/word2vec_clean.model\") \n",
        "\n",
        "print(\"Succesfully loaded all models/arrays/dicts\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6fT9K14ghJq"
      },
      "source": [
        "# Loading the BATS dataset\n",
        "\n",
        "directory = './BATS_3.0'\n",
        "names = []\n",
        "pairs_sets = []\n",
        "\n",
        "for d in os.listdir(directory):\n",
        "    if d != 'metadata.json':\n",
        "        for f in os.listdir(os.path.join(directory,str(d))):\n",
        "            names.append(str(f)[:-4])\n",
        "            pairs_sets.append(set())\n",
        "            with utils.open_file(os.path.join(directory,str(d),str(f))) as fin:\n",
        "                for line_no, line in enumerate(fin):\n",
        "                    line = utils.to_unicode(line)\n",
        "                    a, b = [word.lower() for word in line.split()]\n",
        "                    list_b = b.split('/')\n",
        "                    if list_b[0] != a: #Keeping only the first analogy pair\n",
        "                        pairs_sets[-1].add((a, list_b[0]))\n",
        "\n",
        "pairs_sets = [list(d) for d in pairs_sets]\n",
        "\n",
        "print(\"Succesfully loaded the BATS dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXE2Hc2zghJr"
      },
      "source": [
        "# Matrices W,C and Ct (pseudo-inverse) of the w2v model\n",
        "Ct = np.linalg.pinv(model.syn1neg)\n",
        "W = model.wv.vectors\n",
        "C = model.syn1neg\n",
        "print(\"Extracted the W, C and Ct matrices of the w2v model.\")\n",
        "\n",
        "# Vocabulary definition, inversed vocabulary etc...\n",
        "\n",
        "# For memory purposes, we recommend to limit the vocabulary for tests. \n",
        "# Replace by None if you wish to use the entirety of the vocabulary. \n",
        "vocab_limit = 2000\n",
        "\n",
        "vocabulary_keys = model.wv.index_to_key\n",
        "vocabulary = set(vocabulary_keys)\n",
        "vocabulary_list = np.array(list(vocabulary_keys[:vocab_limit]))\n",
        "len_vocabulary = len(list(vocabulary_keys[:vocab_limit]))\n",
        "\n",
        "invdict_vocabulary = dict.fromkeys(vocabulary_list)\n",
        "for i,wi in enumerate(vocabulary_list):\n",
        "    invdict_vocabulary[wi] = i\n",
        "\n",
        "full_invdict_vocabulary = dict.fromkeys(np.array(list(vocabulary_keys)))\n",
        "for i,wi in enumerate(np.array(list(vocabulary_keys))):\n",
        "    full_invdict_vocabulary[wi] = i\n",
        "    \n",
        "# Top indexes vocabulary\n",
        "count_dict_i = dict.fromkeys(vocabulary_list)\n",
        "for w in count_dict_ij:\n",
        "    count_dict_i[w] = sum(count_dict_ij[w].values())\n",
        "    \n",
        "n_max = 10000    \n",
        "sorted_count_dict = {k: v for k, v in sorted(count_dict_i.items(), key=lambda item: item[1])}\n",
        "sorted_vocabulary = list(reversed([[k, v] for (k, v) in sorted_count_dict.items()]))\n",
        "top_voc_list = [w[0] for w in sorted_vocabulary[:n_max]]\n",
        "top_voc = set(top_voc_list)\n",
        "invdict_top_voc = {w:i for i,w in enumerate(top_voc_list)}\n",
        "    \n",
        "sorted_count_dict_full = {k: v for k, v in sorted(count_dict_i.items(), key=lambda item: item[1])}\n",
        "sorted_vocabulary_full = list(reversed([[k, v] for (k, v) in sorted_count_dict.items()]))\n",
        "top_voc_list_full = [w[0] for w in sorted_vocabulary]\n",
        "top_voc_full = set(top_voc_list)\n",
        "invdict_top_voc_full = {w:i for i,w in enumerate(top_voc_list)}\n",
        "    \n",
        "# Relation between vocabulary indexes and top_vocabulary indexes (switching between w2v and pmi indexes)\n",
        "#     n_i[i] = index for word v in pmi for v = to w2v_dict[i]\n",
        "#     pmi[n_i[i]] -> pmi[i] = pmi of word w2v_dict[i]\n",
        "new_idxs_array = np.array([full_invdict_vocabulary[model.wv.index_to_key[i]] \n",
        "                           for i in range(len(model.wv.index_to_key))]).astype(np.int)\n",
        "\n",
        "new_idxs_inv_array = np.zeros(new_idxs_array.shape)\n",
        "for i in range(new_idxs_array.shape[0]):\n",
        "    new_idxs_inv_array[new_idxs_array[i]] = int(i)\n",
        "new_idxs_inv_array = new_idxs_inv_array.astype(np.int)\n",
        "    \n",
        "print(\"Prepared the vocabulary and other important data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMRptZRFghJs"
      },
      "source": [
        "# idx <-> word tuple relation of the paragraph error matrix\n",
        "def idx_oftuple(w1,w2, invdict_top_voc, n=10000):\n",
        "    if invdict_top_voc[w2] < invdict_top_voc[w1]:\n",
        "        w1, w2 = w2, w1\n",
        "    return(n*invdict_top_voc[w1]+invdict_top_voc[w2])\n",
        "\n",
        "def tuple_ofidx(idx, top_voc_list, n=10000):\n",
        "    i1, i2 = idx//n, idx%n\n",
        "    return(top_voc_list[i1], top_voc_list[i2])\n",
        "\n",
        "def ww_sim_analogy(positive, negative, mat, tok2indx, indx2tok, topn=10, numpy=False):\n",
        "    \"\"\"Calculate topn most similar words to word\"\"\"\n",
        "    if numpy:\n",
        "        positives = [mat[tok2indx[indx]] for indx in positive]\n",
        "        negatives = [mat[tok2indx[indx]] for indx in negative]\n",
        "        v1 = np.copy(positives[0])\n",
        "    else:\n",
        "        positives = [mat.getrow(tok2indx[indx]) for indx in positive]\n",
        "        negatives = [mat.getrow(tok2indx[indx]) for indx in negative]\n",
        "        v1 = scipy.sparse.csr_matrix.copy(positives[0]) \n",
        "    for p in positives[1:]:\n",
        "        v1 += p\n",
        "    for n in negatives:\n",
        "        v1 -= n\n",
        "    sims = cos_sim(mat, v1).flatten()\n",
        "    sindxs = np.argsort(-sims)\n",
        "    sim_word_scores = [(indx2tok[sindx], sims[sindx]) for sindx in sindxs[0:topn]]\n",
        "    return sim_word_scores\n",
        "\n",
        "def ww_sim_analogy_para_rank(positive1, positive2, mat, top_voc_list, invdict_top_voc, topn=10, negative=None, metric='cossim'):\n",
        "    \"\"\"\n",
        "    Calculate topn most similar word sets to a given word set from the paraphrase error. \n",
        "    Can specify a wanted \"negative\" word wanted.\n",
        "    \"\"\"\n",
        "    \n",
        "    if type(negative) == list:\n",
        "        n1, n2 = negative[0], negative[1]\n",
        "    \n",
        "    v1 = mat.getrow(idx_oftuple(positive1, positive2, invdict_top_voc))\n",
        "\n",
        "    sims = pairwise_distances(mat, v1, metric).flatten()\n",
        "    sindxs = np.argsort(-sims)\n",
        "    \n",
        "    value = idx_oftuple(n1, n2, invdict_top_voc)\n",
        "    i, = np.where(sindxs == value)\n",
        "\n",
        "    return(i[0])\n",
        "\n",
        "print('Defined all useful functions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqXHeyXEghJs"
      },
      "source": [
        "# Error terms vectors definitions\n",
        "\n",
        "epsilon = 1e-10\n",
        "\n",
        "# Create the errors terms and their norms\n",
        "def paraphrase_error_vector(w1,w2,w3,w4, invdict_top_voc, n_ww, count_matrix_tri):\n",
        "    p_e = np.zeros(len(vocabulary_list))\n",
        "    \n",
        "    i_w1w2 = idx_oftuple(w1, w2, invdict_top_voc)\n",
        "    i_w3w4 = idx_oftuple(w3, w4, invdict_top_voc)\n",
        "    \n",
        "    n_w1w2 = n_ww[i_w1w2]\n",
        "    n_w3w4 = n_ww[i_w3w4]\n",
        "    \n",
        "    for k, wk in enumerate(vocabulary_list):\n",
        "        #total_count = log_or_null(total_count_paraph(count_dict_cj_mwkq, w1, w2, wk))\n",
        "        n_c_w1w2 = count_matrix_tri[i_w1w2, invdict_vocabulary[wk]]\n",
        "        n_c_w3w4 = count_matrix_tri[i_w3w4, invdict_vocabulary[wk]]\n",
        "        \n",
        "        if n_c_w1w2 == 0:\n",
        "            if n_c_w3w4 == 0:\n",
        "                p_e_k = 0\n",
        "            else:\n",
        "                p_e_k = np.log(epsilon)\n",
        "        else:\n",
        "            if n_c_w3w4 == 0:\n",
        "                p_e_k = -np.log(epsilon)\n",
        "            else:\n",
        "                p_e_k = np.log((n_c_w1w2 * n_w3w4)/(n_c_w3w4 * n_w1w2))\n",
        "        p_e[k] = p_e_k\n",
        "        \n",
        "    return(p_e)\n",
        "\n",
        "def indep_error_vector(w1,w2,w3,w4, invdict_top_voc, n_ww, n_w, n_w_c, count_matrix_tri):\n",
        "    i_e = np.zeros(len(vocabulary_list))\n",
        "    \n",
        "    i_w1w2 = idx_oftuple(w1, w2, invdict_top_voc)\n",
        "    i_w3w4 = idx_oftuple(w3, w4, invdict_top_voc)\n",
        "    \n",
        "    n_w1w2 = n_ww[i_w1w2]\n",
        "    n_w3w4 = n_ww[i_w3w4]\n",
        "    n_w1 = n_w[w1]\n",
        "    n_w2 = n_w[w2]\n",
        "    n_w3 = n_w[w3]\n",
        "    n_w4 = n_w[w4]\n",
        "    \n",
        "    tau = np.log((n_w3w4*n_w1*n_w2)/(n_w1w2*n_w3*n_w4))\n",
        "    \n",
        "    for k, wk in enumerate(vocabulary_list):\n",
        "        n_c_w1w2 = count_matrix_tri[i_w1w2, invdict_vocabulary[wk]]\n",
        "        n_c_w3w4 = count_matrix_tri[i_w3w4, invdict_vocabulary[wk]]\n",
        "        \n",
        "        n_c_w1 = n_w_c[w1].get(wk,0)\n",
        "        n_c_w2 = n_w_c[w2].get(wk,0)\n",
        "        n_c_w3 = n_w_c[w3].get(wk,0)\n",
        "        n_c_w4 = n_w_c[w4].get(wk,0)\n",
        "        \n",
        "        if n_c_w1w2 == 0:\n",
        "            if n_c_w1 == 0 or n_c_w2 == 0:\n",
        "                i_e_w = 0\n",
        "            else:\n",
        "                i_e_w = np.log(epsilon)\n",
        "        else:\n",
        "            if n_c_w1 == 0 or n_c_w2 == 0:\n",
        "                # Impossible case normally\n",
        "                i_e_w = -np.log(epsilon)\n",
        "            else:\n",
        "                i_e_w = np.log((n_c_w1w2)/(n_c_w1*n_c_w2))\n",
        "                \n",
        "        if n_c_w3w4 == 0:\n",
        "            if n_c_w3 == 0 or n_c_w4 == 0:\n",
        "                i_e_we = 0\n",
        "            else:\n",
        "                i_e_we = -np.log(epsilon)\n",
        "        else:\n",
        "            if n_c_w3 == 0 or n_c_w4 == 0:\n",
        "                print(\"erreur?\")\n",
        "                i_e_we = np.log(epsilon)\n",
        "            else:\n",
        "                i_e_we = -np.log((n_c_w3w4)/(n_c_w3*n_c_w4))\n",
        "                \n",
        "        i_e[k] = i_e_w + i_e_we + tau\n",
        "        \n",
        "    return(i_e)\n",
        "\n",
        "def normes(p_e, i_e):\n",
        "    n_p, n_i, n_s = np.linalg.norm(p_e), np.linalg.norm(i_e), np.linalg.norm(p_e + i_e)\n",
        "    n_p_1, n_i_1, n_s_1 = np.linalg.norm(p_e, ord=1), np.linalg.norm(i_e, ord=1), np.linalg.norm(p_e + i_e, ord=1)\n",
        "    return([n_p, n_i, n_s, n_p_1, n_i_1, n_s_1])\n",
        "\n",
        "def errors_norms(w1,w2,w3,w4, invdict_top_voc, n_w1w2,  n_w, n_w_c, count_matrix_tri):\n",
        "    p_e = paraphrase_error_vector(w1,w2,w3,w4, invdict_top_voc, n_w1w2, count_matrix_tri)\n",
        "    i_e = indep_error_vector(w1,w2,w3,w4, invdict_top_voc, n_w1w2, n_w, n_w_c, count_matrix_tri)\n",
        "    return(normes(p_e, i_e))\n",
        "\n",
        "print('Defined the error terms functions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "outjFy3_ghJt"
      },
      "source": [
        "# Link between the PMI and word2vec embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQN_lOir_6fc"
      },
      "source": [
        "Reconstruction of the PMI vector and word2vec embedding for a given word.\r\n",
        "Here the word chosen is King. We can find a relatively high correlation, but not good enough for concluding that the link is linear enough, especially since this is done for the closest word2vec model possible to the PMI factorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4fP4jwFghJv"
      },
      "source": [
        "font = {'family' : 'serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 28}\n",
        "\n",
        "iking = model.wv.index_to_key.index('king')\n",
        "false_pmi_king = np.dot(W[new_idxs_inv_array[iking]][:pmi_matrix.shape[0]],\n",
        "                        C[new_idxs_inv_array[:pmi_matrix.shape[0]]].T)\n",
        "false_w2v_emb_king = np.dot(Ct[:,new_idxs_inv_array[:pmi_matrix.shape[0]]],\n",
        "                            pmi_matrix[invdict_vocabulary['king']].toarray()[0])\n",
        "\n",
        "x,y = false_pmi_king, pmi_matrix[invdict_vocabulary['king']].toarray()[0]\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x, y, s=1)\n",
        "plt.xlabel('Values of the reconstructed PMI vector')\n",
        "plt.ylabel('Values of the true PMI vector')\n",
        "plt.rc('font', **font)\n",
        "plt.show()\n",
        "\n",
        "print(scipy.stats.pearsonr(x, y))\n",
        "\n",
        "x,y = model.wv.get_vector('king'), false_w2v_emb_king\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(x, y, s=20)\n",
        "plt.xlabel('Values of the true w2v vector')\n",
        "plt.ylabel('Values of the reconstructed w2v vector')\n",
        "plt.rc('font', **font)\n",
        "plt.show()\n",
        "\n",
        "print(scipy.stats.pearsonr(x, y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neq2pSrZmAVZ"
      },
      "source": [
        "# Compute the correlations for the top words in the vocabulary (10k by default)\r\n",
        "\r\n",
        "correlations = []\r\n",
        "\r\n",
        "Ct_rearranged = Ct[:,new_idxs_inv_array]\r\n",
        "\r\n",
        "for i in range(len(top_voc_list)):\r\n",
        "  if i%1000==0: print(i)\r\n",
        "  top_voc_list\r\n",
        "  wi = top_voc_list[i]\r\n",
        "  false_w2v_emb_wi = np.dot(Ct_rearranged,\r\n",
        "                            pmi_matrix[invdict_vocabulary[wi]].toarray()[0])\r\n",
        "\r\n",
        "\r\n",
        "  correlations.append(scipy.stats.pearsonr(model.wv.get_vector(wi), false_w2v_emb_wi)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2D9SU3Urk9A"
      },
      "source": [
        "import plotly.graph_objects as go\r\n",
        "\r\n",
        "xaxis_title_text = 'Correlation between true and approximated embedding'\r\n",
        "yaxis_title_text = 'Count'\r\n",
        "\r\n",
        "fig = go.Figure(data=[go.Histogram(x=correlations)])\r\n",
        "fig.update_layout(\r\n",
        "    font=dict(\r\n",
        "        family=\"Times New Roman\",\r\n",
        "        size=30),\r\n",
        "    xaxis_title_text=xaxis_title_text,\r\n",
        "    yaxis_title_text=yaxis_title_text)\r\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mfu-TD5ghJx"
      },
      "source": [
        "# Percentages of paraphrases cooccuring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTwDXorFghJx"
      },
      "source": [
        "occurences_para_p = []\n",
        "occurences_para_n = []\n",
        "occurences_para_not_in_corpus = []\n",
        "\n",
        "for k in range(len(pairs_sets)):\n",
        "    positive = 0\n",
        "    negative = 0\n",
        "    not_in_corpus = 0\n",
        "    \n",
        "    for i in range(len(pairs_sets[k])):\n",
        "        p1_a, p1_ap = pairs_sets[k][i]\n",
        "        for j in range(i+1, len(pairs_sets[k])):\n",
        "            p2_a, p2_ap = pairs_sets[k][j]\n",
        "            \n",
        "            if (p1_a in count_dict_ij and p2_ap in count_dict_ij[p1_a]) or (p2_ap in count_dict_ij and p1_a in count_dict_ij[p2_ap]):\n",
        "                positive += 1\n",
        "            else:\n",
        "                if p1_a in count_dict_i and p2_ap in count_dict_i:\n",
        "                    negative += 1\n",
        "                else:\n",
        "                    not_in_corpus += 1\n",
        "            if (p2_a in count_dict_ij and p1_ap in count_dict_ij[p2_a]) or (p1_ap in count_dict_ij and p2_a in count_dict_ij[p1_ap]):\n",
        "                positive += 1\n",
        "            else:\n",
        "                if p2_a in count_dict_i and p1_ap in count_dict_i:\n",
        "                    negative += 1\n",
        "                else:\n",
        "                    not_in_corpus += 1\n",
        "                \n",
        "            \n",
        "    occurences_para_p.append(positive)\n",
        "    occurences_para_n.append(negative)\n",
        "    occurences_para_not_in_corpus.append(not_in_corpus) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vYXGIIFghJx"
      },
      "source": [
        "for k in range(len(pairs_sets)):\n",
        "    p = occurences_para_p[k]\n",
        "    n = occurences_para_n[k]\n",
        "    nic = occurences_para_not_in_corpus[k]\n",
        "    total = p+n+nic\n",
        "    print(names[k][:3],\n",
        "          '; Paraph. with cooc.: ', float(int(p/total*10000))/100,\n",
        "          '%; Paraph. with NO cooc.: ', float(int(n/total*10000))/100,\n",
        "          '%; Paraph. with no oc. of 1 of the words: ', float(int(nic/total*10000))/100, '%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C2SQvYPghJy"
      },
      "source": [
        "# Analogies (W and W*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isOLq-a2ghJy"
      },
      "source": [
        "# Creation of the suitable W and W* pairs.\n",
        "\n",
        "def possible_analogies_and_voc(pairs_sets, count_dict_ij, invdict_top_voc):\n",
        "  possible_analogies = []\n",
        "  for k in range(len(pairs_sets)):\n",
        "      possible_analogies.append([])\n",
        "      for i in range(len(pairs_sets[k])):\n",
        "          p1_a, p1_ap = pairs_sets[k][i]\n",
        "          for j in range(i+1, len(pairs_sets[k])):\n",
        "              p2_a, p2_ap = pairs_sets[k][j]\n",
        "\n",
        "              if (p1_a in count_dict_ij and p2_ap in count_dict_ij[p1_a]) or (p2_ap in count_dict_ij and p1_a in count_dict_ij[p2_ap]):\n",
        "                  if (p2_a in count_dict_ij and p1_ap in count_dict_ij[p2_a]) or (p1_ap in count_dict_ij and p2_a in count_dict_ij[p1_ap]):\n",
        "                      if p1_a in invdict_top_voc and p1_ap in invdict_top_voc and p2_ap in invdict_top_voc and p2_a in invdict_top_voc:\n",
        "                          possible_analogies[-1].append([(p1_a, p2_ap), (p1_ap, p2_a)])\n",
        "                          \n",
        "  voc_possible_analogies = set()\n",
        "  for k in range(len(pairs_sets)):\n",
        "      for p in possible_analogies[k]:\n",
        "          t1, t2 = p[0], p[1]\n",
        "          p0,p1 = t1\n",
        "          n0,n1 = t2\n",
        "          voc_possible_analogies.add(p0)\n",
        "          voc_possible_analogies.add(p1)\n",
        "          voc_possible_analogies.add(n0)\n",
        "          voc_possible_analogies.add(n1)\n",
        "  voc_possible_analogies = list(voc_possible_analogies)\n",
        "\n",
        "  return(possible_analogies, voc_possible_analogies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezbbi2pjscds"
      },
      "source": [
        "possible_analogies, voc_possible_analogies = possible_analogies_and_voc(pairs_sets, count_dict_ij, invdict_top_voc)\r\n",
        "\r\n",
        "# Examples of paraphrases W and W* forming analogies in the male-female category.\r\n",
        "print(possible_analogies[16])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-WtcgelLghJz"
      },
      "source": [
        "print('Number of possibles analogies using paraphrases, where W and W* both are coocuring; using only the top 10 000 words')\n",
        "\n",
        "for i,p in enumerate(possible_analogies):\n",
        "    print(names[i], str(len(p)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92k79EMOghJ0"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Srb1fUPNghJ0"
      },
      "source": [
        "# Ranks of the country_capital category for the paraphrase matrix distance\n",
        "\n",
        "# The vectors [p(cj|W)]_j have coordinates equal to 0 when N(W,cj) == 0, \n",
        "# which differs slightly from the previous paraphrase definition; but is necessary for computation purposes\n",
        "\n",
        "# Choose all BATS categories or some in particular\n",
        "all_categories = False\n",
        "\n",
        "if all_categories:\n",
        "    categories = list(range(len(names)))\n",
        "else:\n",
        "    k_female = 16\n",
        "    k_country = 19\n",
        "    categories = [k_female, k_country]\n",
        "\n",
        "ranks_total= []\n",
        "for k in categories:\n",
        "    ranks_total.append([])\n",
        "    for ana in possible_analogies[k]:\n",
        "        t1, t2 = ana[0], ana[1]\n",
        "        p0,p1 = t1\n",
        "        n0,n1 = t2\n",
        "        ranks_total[-1].append(ww_sim_analogy_para_rank(p0, p1, para_matrix, top_voc_list_full, invdict_top_voc_full, metric='l2', negative = [n0, n1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clqNfYZ2ghJ0"
      },
      "source": [
        "# Print the mean and median rank of every category\n",
        "\n",
        "for ik,k in enumerate(categories):\n",
        "    print(names[k], ': Mean= ', np.mean(ranks_total[ik]), ' ; Median= ', np.median(ranks_total[ik]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cHazZJghJ2"
      },
      "source": [
        "# Norms computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoZW9C0wghJ3"
      },
      "source": [
        "# Preparations to compute norms\n",
        "# n_w_c is long to compute and is thus computed in the previous notebook.\n",
        "\n",
        "# Count_matrix_tri = N(w1, w2, cj)\n",
        "\n",
        "# N(w1,w2)\n",
        "n_w1w2 = count_matrix_tri.sum(axis=1)\n",
        "n_w1w2 =np.array(n_w1w2)[:,0]\n",
        "\n",
        "# N(cj)\n",
        "n_cj = count_matrix_tri.sum(axis=0)\n",
        "\n",
        "# N(w1)\n",
        "n_w = dict()\n",
        "for wi in voc_possible_analogies:\n",
        "    n_wi = 0\n",
        "    for wj in top_voc_list:\n",
        "        if invdict_top_voc[wi] > invdict_top_voc[wj]:\n",
        "            n_wi += int(n_w1w2[10000*invdict_top_voc[wj] + invdict_top_voc[wi]])\n",
        "        else:\n",
        "            n_wi += int(n_w1w2[10000*invdict_top_voc[wi] + invdict_top_voc[wj]])\n",
        "    n_w[wi] = n_wi\n",
        "    \n",
        "# N(w1, cj)\n",
        "\n",
        "n_w_c = load_obj( \"./Models/n_w_c\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-et88rIjhonA"
      },
      "source": [
        "norms_types = [\"Paraphrase error L2\", \"Dependence errors L2\", \"All errors L2\", \r\n",
        "               \"Paraphrase error L1\", \"Dependence errors L1\", \"All errors L1\"]\r\n",
        "\r\n",
        "# Choose all BATS categories or some in particular\r\n",
        "all_categories = False\r\n",
        "\r\n",
        "if all_categories:\r\n",
        "    categories = list(range(len(names)))\r\n",
        "else:\r\n",
        "    k_female = 16\r\n",
        "    k_country = 19\r\n",
        "    categories = [k_female, k_country]\r\n",
        "\r\n",
        "normes_total= []\r\n",
        "for k in categories:\r\n",
        "    normes_total.append([])\r\n",
        "    for i_ana, ana in enumerate(possible_analogies[k]):\r\n",
        "        t1, t2 = ana[0], ana[1]\r\n",
        "        p0,p1 = t1\r\n",
        "        n0,n1 = t2\r\n",
        "        normes_total[-1].append(errors_norms(p0,p1,n0,n1, invdict_top_voc, n_w1w2,  n_w, n_w_c, count_matrix_tri))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBykFQ-Khrxo"
      },
      "source": [
        "# Print the mean and median norms for every category\r\n",
        "\r\n",
        "for ik,k in enumerate(categories):\r\n",
        "    print(names[k])\r\n",
        "    for i,l in enumerate(np.array(normes_total[ik]).T):\r\n",
        "            print(norms_types[i], ': Mean= ', np.mean(l), ' ; Median= ', np.median(l))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4AwN7g_ghJ4"
      },
      "source": [
        "# Context words in cooccurences of a paraphrase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0TLKo_zghJ4"
      },
      "source": [
        "# Print all the context phrases where king and woman appear together\n",
        "\n",
        "def check_context_of_set(text8, vocabulary, w1, w2):\n",
        "    set_ctxt = set()\n",
        "    for nd,doc in enumerate(text8):\n",
        "        for i,wi in enumerate(doc):\n",
        "            for j in range(max(0,i-5),min(len(doc),i+6)):\n",
        "                cj = doc[j]\n",
        "                for k in range(j+1,min(len(doc),i+6)):\n",
        "                    ck = doc[k]\n",
        "                    if (cj == w1 and ck == w2) or (cj == w2 and ck == w1) and wi in vocabulary and j != i and k != i:\n",
        "                        for wc in doc[i-5:i+6]:\n",
        "                            set_ctxt.add(wc)\n",
        "    return(set_ctxt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8pATZTCghJ4"
      },
      "source": [
        "#set_context_kw = check_context_of_set(text8, vocabulary, 'king', 'woman')\n",
        "#set_context_qm = check_context_of_set(text8, vocabulary, 'queen', 'man')\n",
        "\n",
        "# For a context of 5, the computed context words are:\n",
        "set_context_kw = {\n",
        " 'a','about','alfred','ammonite','an','and','apologized','as','assigns','at','be','beautiful','beget','bells','born','brought','but','by',\n",
        " 'clad','consequently','danish','day','died','donald','edmund','elf','encourage','established','evil','exactly','famed','final','finds','foreign',\n",
        " 'from','grimhild','had','happens','have','he','heart','helgi','her','herself','high','his','i','identity','in','insisted','is','king',\n",
        " 'kings','la','line','lizard','marriage','married','murdered','named','of','official','offspring','olympias','on','one','over',\n",
        " 'pass','peirithous','philip','plottest','priced','profusely','queen','realizing','refrain','relationship','resurrection','return',\n",
        " 'returns','s','saga','says','servant','she','short','silk','single','solomon','son','stomach','task','telling','term','that','the',\n",
        " 'thessalian','thou','to','two','upon','was','where','who','will','wise','with','woman','wouldst','xiv'}\n",
        "\n",
        "set_context_qm = {\n",
        " 'a','adelaide','all','an','and','anointed','around','as','celebrating','celebrations','channel','charge','crowning','dancing','day',\n",
        " 'deputy','for','green','her','his','in','include','is','islands','isle','iv','james','k','karaoke','king','kingdom','man','mary','may',\n",
        " 'maypole','more','morris','much','of','office','prosecution','queen','representative','s','scots','seasons','she','simpler','since','singing',\n",
        " 'sophisticated','tastes','the','thomas','try','ultimately','united','was','who','wife','william','with'}\n",
        "\n",
        "# Words in context presented at the same time as \"King\" and \"Woman\".\n",
        "print(set(set_context_qm).intersection(set(set_context_kw)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}